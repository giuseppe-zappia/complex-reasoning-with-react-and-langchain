{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8769ba-5cdb-4e55-84b9-e3fa0f7e8cac",
   "metadata": {},
   "source": [
    "# Enabling Complex Reasoning and Action with ReAct, LLMs, and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fdda37-63cb-4a61-89ec-0b7900ae83d2",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to use multiple different techniques and models to build a ReAct based framework. ReAct is an approach to problem solving with large language models based on 2 main premises: Reasoning and Action. With ReAct, you combine reasoning, through chain-of-thought, with the ability to perform actions through a set of tools. This enables the model to (Re)ason through the input request to determine what steps need to be performed, and uses the available tools to perform (ACT)ions as part of a step-by-step resolution.\n",
    "\n",
    "More details on ReAct can be found in this research paper: [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629) and the [Google AI Blog](https://blog.research.google/2022/11/react-synergizing-reasoning-and-acting.html)\n",
    "\n",
    "![image.png](ReAct-Diagram.png)\n",
    "\n",
    "Image taken from Google AI Blog\n",
    "\n",
    "To demonstrate the potential of ReAct this notebook will focus on a use case involving and Insurance Bot.  For demonstration purposes, this Bot is designed to handle insurance policy requests and is provided a set of tools, including a SQLLite database and an insurance processing API, to accept requests on input, reason through the steps needed to carry out the request, and carry out the actions required.  \n",
    "\n",
    "![image.png](./images/Overview-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c9b99-de64-4829-ba40-079b58721faa",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a11660-abf0-44ff-b0d9-e91a36ca95f1",
   "metadata": {},
   "source": [
    "As part of this workshop you will run a local, quantized version of Meta's LLaMa-2-13B. In order do do this you will need llama-cpp-python, which requires a local compiler. This will install `g++` and set it as the default so that you can install llama-cpp-python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e030aa9a-3e86-47a9-a031-928e51ebfe34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets==8.1.2 in /opt/conda/lib/python3.10/site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.2) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.2) (8.22.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.2) (5.14.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.2) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.2) (3.0.10)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.2) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.1.2) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets==8.1.2) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.2) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.2) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.2) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets==8.1.2) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.39.3 (from transformers[torch]==4.39.3)\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3->transformers[torch]==4.39.3) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3->transformers[torch]==4.39.3) (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3->transformers[torch]==4.39.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3->transformers[torch]==4.39.3) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3->transformers[torch]==4.39.3) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3->transformers[torch]==4.39.3) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3->transformers[torch]==4.39.3) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3->transformers[torch]==4.39.3)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3->transformers[torch]==4.39.3) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3->transformers[torch]==4.39.3) (4.66.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]==4.39.3) (2.0.0.post101)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]==4.39.3) (0.21.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]==4.39.3) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3->transformers[torch]==4.39.3) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3->transformers[torch]==4.39.3) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]==4.39.3) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]==4.39.3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]==4.39.3) (3.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3->transformers[torch]==4.39.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3->transformers[torch]==4.39.3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3->transformers[torch]==4.39.3) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3->transformers[torch]==4.39.3) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]==4.39.3) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]==4.39.3) (1.3.0)\n",
      "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.31.0\n",
      "    Uninstalling transformers-4.31.0:\n",
      "      Successfully uninstalled transformers-4.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.2 requires transformers[sentencepiece]<4.32.0,>=4.31.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.39.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentence_transformers==2.6.1\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.6.1) (4.39.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.6.1) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.6.1) (2.0.0.post101)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.6.1) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.6.1) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.6.1) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.6.1) (0.22.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.6.1) (9.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers==2.6.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers==2.6.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers==2.6.1) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers==2.6.1) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers==2.6.1) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers==2.6.1) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.6.1) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.6.1) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers==2.6.1) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.6.1) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers==2.6.1) (1.3.0)\n",
      "Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain==0.1.16\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (1.4.49)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain==0.1.16)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain==0.1.16)\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.16)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (0.1.31)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.16) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.16) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain==0.1.16) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.9.15)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.16) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.16) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.16) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.16) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.16) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (1.0.0)\n",
      "Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.33\n",
      "    Uninstalling langchain-core-0.1.33:\n",
      "      Successfully uninstalled langchain-core-0.1.33\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.29\n",
      "    Uninstalling langchain-community-0.0.29:\n",
      "      Successfully uninstalled langchain-community-0.0.29\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.9\n",
      "    Uninstalling langchain-0.1.9:\n",
      "      Successfully uninstalled langchain-0.1.9\n",
      "Successfully installed langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.45 langchain-text-splitters-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting faiss-cpu==1.8.0\n",
      "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from faiss-cpu==1.8.0) (1.26.4)\n",
      "Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain-experimental==0.0.57\n",
      "  Downloading langchain_experimental-0.0.57-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.15 in /opt/conda/lib/python3.10/site-packages (from langchain-experimental==0.0.57) (0.1.16)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.41 in /opt/conda/lib/python3.10/site-packages (from langchain-experimental==0.0.57) (0.1.45)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.4.49)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.0.34)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.1.31)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (8.2.3)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.41->langchain-experimental==0.0.57) (23.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.9.15)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.0.0)\n",
      "Downloading langchain_experimental-0.0.57-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-experimental\n",
      "Successfully installed langchain-experimental-0.0.57\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sqlalchemy==2.0.29\n",
      "  Downloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting typing-extensions>=4.6.0 (from sqlalchemy==2.0.29)\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy==2.0.29) (3.0.3)\n",
      "Downloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: typing-extensions, sqlalchemy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.49\n",
      "    Uninstalling SQLAlchemy-1.4.49:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.49\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dash 2.16.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.16.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.16.1 requires dash-table==5.0.0, which is not installed.\n",
      "jupyter-scheduler 2.5.1 requires sqlalchemy~=1.0, but you have sqlalchemy 2.0.29 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\n",
      "tensorflow 2.12.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sqlalchemy-1.4.51 typing-extensions-4.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting json2html==1.3.0\n",
      "  Downloading json2html-1.3.0.tar.gz (7.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: json2html\n",
      "  Building wheel for json2html (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for json2html: filename=json2html-1.3.0-py3-none-any.whl size=7593 sha256=ccfd97a01c251e83f3b7c81ddf76731dd33d15fc757ab7b12cf1e7e22175e39a\n",
      "  Stored in directory: /home/sagemaker-user/.cache/pip/wheels/e0/d8/b3/6f83a04ab0ec00e691de794d108286bb0f8bcdf4ade19afb57\n",
      "Successfully built json2html\n",
      "Installing collected packages: json2html\n",
      "Successfully installed json2html-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.10/site-packages (from numexpr) (1.26.4)\n",
      "Downloading numexpr-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.1/376.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numexpr\n",
      "Successfully installed numexpr-2.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets==8.1.2\n",
    "%pip install transformers[torch]==4.39.3\n",
    "%pip install sentence_transformers==2.6.1\n",
    "%pip install langchain==0.1.16\n",
    "%pip install faiss-cpu==1.8.0\n",
    "%pip install langchain-experimental==0.0.57\n",
    "%pip install sqlalchemy==2.0.29\n",
    "%pip install json2html==1.3.0\n",
    "%pip install numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fff23-2268-454e-bf32-31d3a6433166",
   "metadata": {},
   "source": [
    "---\n",
    "# ========>  IMPORTANT!! <============\n",
    "\n",
    "Please restart your notebook kernel by either clicking the refresh button in the notebook menu bar or going to Kernel > Restart Kernel .  You can then resume the next steps in your notebook starting with the cell below. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02fb72-f2e7-4a70-b62c-cd015548a667",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bedrock Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86abfdde-e12a-48d4-8fef-5c9cc8ac9a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "bedrock_client = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2263d-b376-4aee-a4b5-6962891d9b7b",
   "metadata": {},
   "source": [
    "# Workshop Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06870628-2f15-4da1-84fd-2464ac913a42",
   "metadata": {
    "tags": []
   },
   "source": [
    "Because we'll be using LangChain for this workshop, we'll need to import the library to start..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7229813-f772-4e85-80bb-9a060d1d857e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0e591-2f09-415f-8732-45cbbbe6c8a8",
   "metadata": {},
   "source": [
    "## Setup sample data using SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f981bd-9e1e-4300-88c7-b321c2ec8639",
   "metadata": {
    "tags": []
   },
   "source": [
    "As part of this workshop, you will see how to work with a variety of different tool types to retrieve and act on data.\n",
    "\n",
    "Here you will create an in-memory SQLite database to store some sample data for the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f39b8f-0aa9-4600-9bf2-553dcca45d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5440ac-1da0-4f5c-b321-b5c3102030f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Build out a table of insurance policies. This would typically be normalized, but for simplicity's sake of this example, the policies and users are all in 1 table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c276bfd9-e896-4574-826b-be13cbfd1340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Table, Date\n",
    "\n",
    "policies = Table(\n",
    "    \"policies\",\n",
    "    metadata_obj,\n",
    "    Column(\"policy_id\", Integer, primary_key=True),\n",
    "    Column(\"first_name\", String(50), nullable=False),\n",
    "    Column(\"last_name\", String(50), nullable=False),\n",
    "    Column(\"phone\", String(15), nullable=False),\n",
    "    Column(\"policy_type\", String(25), nullable=False),\n",
    "    Column(\"policy_date\", Date, nullable=False),\n",
    "    Column(\"policy_value\", Integer, nullable=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47fef6d-d9f1-4004-890d-f85533b39fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "sqllite_engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj.create_all(sqllite_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f154acc-50c3-4d02-9798-8a7c9304d8d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generate some some dummy data and insert it into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79677473-6862-4545-b774-efd8cb62f606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#policy data: policy_id, first_name, last_name, phone, policy_type, policy_date, policy_value\n",
    "\n",
    "policy_data = [\n",
    "    [48918, 'Ernest', 'Mcneil', '349-711-8757', 'home', datetime(2023, 1, 1), 250000],\n",
    "    [66958, 'Brian', 'Patel', '368-889-1742', 'auto', datetime(2023, 1, 2), 32000],\n",
    "    [21947, 'Bertram', 'Mcgee', '798-641-5925', 'home', datetime(2023, 1, 3), 550000],\n",
    "    [17108, 'Margarito', 'Rollins', '348-321-5711', 'auto', datetime(2023, 1, 4), 75000],\n",
    "    [98362, 'Miriam', 'Sutton', '361-863-4332', 'auto', datetime(2023, 1, 8), 21000],\n",
    "    [17565, 'Charmaine', 'Hopkins', '206-566-6359', 'home', datetime(2023, 1, 2), 135000],\n",
    "    [10157, 'Jewel', 'Ingram', '598-338-6133', 'home', datetime(2023, 1, 6), 750000],\n",
    "    [33372, 'Kaye', 'Underwood', '555-720-3848', 'home', datetime(2023, 1, 1), 235000],\n",
    "    [97143, 'Josiah', 'Vazquez', '211-391-1757', 'auto', datetime(2023, 1, 5), 17250],\n",
    "    [54621, 'Charles', 'Wise', '502-236-0425', 'home', datetime(2023, 1, 4), 1592000],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517c127a-5bcf-4252-bbf6-9f7c993f7793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "\n",
    "def insert_policy_data(policy_data_arr):\n",
    "    stmt = insert(policies).values(\n",
    "    policy_id=policy_data_arr[0],\n",
    "    first_name=policy_data_arr[1],\n",
    "    last_name=policy_data_arr[2],\n",
    "    phone=policy_data_arr[3],\n",
    "    policy_type=policy_data_arr[4],\n",
    "    policy_date=policy_data_arr[5],\n",
    "    policy_value=policy_data_arr[6]\n",
    "    )\n",
    "\n",
    "    with sqllite_engine.begin() as conn:\n",
    "        conn.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70e78b6e-209d-4cc9-884e-2b6f7a362e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48918, 'Ernest', 'Mcneil', '349-711-8757', 'home', datetime.datetime(2023, 1, 1, 0, 0), 250000]\n",
      "[66958, 'Brian', 'Patel', '368-889-1742', 'auto', datetime.datetime(2023, 1, 2, 0, 0), 32000]\n",
      "[21947, 'Bertram', 'Mcgee', '798-641-5925', 'home', datetime.datetime(2023, 1, 3, 0, 0), 550000]\n",
      "[17108, 'Margarito', 'Rollins', '348-321-5711', 'auto', datetime.datetime(2023, 1, 4, 0, 0), 75000]\n",
      "[98362, 'Miriam', 'Sutton', '361-863-4332', 'auto', datetime.datetime(2023, 1, 8, 0, 0), 21000]\n",
      "[17565, 'Charmaine', 'Hopkins', '206-566-6359', 'home', datetime.datetime(2023, 1, 2, 0, 0), 135000]\n",
      "[10157, 'Jewel', 'Ingram', '598-338-6133', 'home', datetime.datetime(2023, 1, 6, 0, 0), 750000]\n",
      "[33372, 'Kaye', 'Underwood', '555-720-3848', 'home', datetime.datetime(2023, 1, 1, 0, 0), 235000]\n",
      "[97143, 'Josiah', 'Vazquez', '211-391-1757', 'auto', datetime.datetime(2023, 1, 5, 0, 0), 17250]\n",
      "[54621, 'Charles', 'Wise', '502-236-0425', 'home', datetime.datetime(2023, 1, 4, 0, 0), 1592000]\n"
     ]
    }
   ],
   "source": [
    "for policy in policy_data:\n",
    "    print(policy)\n",
    "    insert_policy_data(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532aee6-46a0-4e6c-a9a7-475791759987",
   "metadata": {},
   "source": [
    "Quick query over the database to show all the rows, sorted by last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "215333c8-cdb0-4a08-88e2-ea4aa6e8184a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT policies.policy_id, policies.first_name, policies.last_name, policies.phone, policies.policy_type, policies.policy_date, policies.policy_value \n",
      "FROM policies ORDER BY policies.last_name\n",
      "\n",
      "(17565, 'Charmaine', 'Hopkins', '206-566-6359', 'home', datetime.date(2023, 1, 2), 135000)\n",
      "(10157, 'Jewel', 'Ingram', '598-338-6133', 'home', datetime.date(2023, 1, 6), 750000)\n",
      "(21947, 'Bertram', 'Mcgee', '798-641-5925', 'home', datetime.date(2023, 1, 3), 550000)\n",
      "(48918, 'Ernest', 'Mcneil', '349-711-8757', 'home', datetime.date(2023, 1, 1), 250000)\n",
      "(66958, 'Brian', 'Patel', '368-889-1742', 'auto', datetime.date(2023, 1, 2), 32000)\n",
      "(17108, 'Margarito', 'Rollins', '348-321-5711', 'auto', datetime.date(2023, 1, 4), 75000)\n",
      "(98362, 'Miriam', 'Sutton', '361-863-4332', 'auto', datetime.date(2023, 1, 8), 21000)\n",
      "(33372, 'Kaye', 'Underwood', '555-720-3848', 'home', datetime.date(2023, 1, 1), 235000)\n",
      "(97143, 'Josiah', 'Vazquez', '211-391-1757', 'auto', datetime.date(2023, 1, 5), 17250)\n",
      "(54621, 'Charles', 'Wise', '502-236-0425', 'home', datetime.date(2023, 1, 4), 1592000)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "stmt = select(policies).order_by(policies.c.last_name)\n",
    "print(f'{stmt}\\n')\n",
    "\n",
    "with sqllite_engine.connect() as conn:\n",
    "    for row in conn.execute(stmt):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568a15a-6a98-4c8f-87d4-7c25e1e0cbc3",
   "metadata": {},
   "source": [
    "# Configure Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3425bc-4444-4a90-8985-30a76ccaad71",
   "metadata": {},
   "source": [
    "To show diversity in approaches, we'll use two models from [Amazon Bedrock](https://aws.amazon.com/bedrock/) in this workshop.  \n",
    "\n",
    "   1. **LLaMa-2-13B-Chat** model will be used to generate SQL to query the database you just made  \n",
    "   2. **Anthropic's Claude V3 Sonnet** will be used as the LLM for the reasoning part of the ReAct approach in the form of a [Langchain Agent](https://python.langchain.com/docs/modules/agents/). This model will be responsible for accepting the request, breaking down the chain of thought reasoning, selecting tools, and formulate a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d34fd9-5b8b-42b8-aad3-a490bf28a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json2html import *\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150bed4b-cc6c-4b9d-b13c-a546f1bc09bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-tg1-large</td></tr><tr><th>modelId</th><td>amazon.titan-tg1-large</td></tr><tr><th>modelName</th><td>Titan Text Large</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-g1-text-02</td></tr><tr><th>modelId</th><td>amazon.titan-embed-g1-text-02</td></tr><tr><th>modelName</th><td>Titan Text Embeddings v2</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>EMBEDDING</li></ul></td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-lite-v1:0:4k</td></tr><tr><th>modelId</th><td>amazon.titan-text-lite-v1:0:4k</td></tr><tr><th>modelName</th><td>Titan Text G1 - Lite</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td><ul><li>FINE_TUNING</li><li>CONTINUED_PRE_TRAINING</li></ul></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-lite-v1</td></tr><tr><th>modelId</th><td>amazon.titan-text-lite-v1</td></tr><tr><th>modelName</th><td>Titan Text G1 - Lite</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-express-v1:0:8k</td></tr><tr><th>modelId</th><td>amazon.titan-text-express-v1:0:8k</td></tr><tr><th>modelName</th><td>Titan Text G1 - Express</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td><ul><li>FINE_TUNING</li><li>CONTINUED_PRE_TRAINING</li></ul></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-express-v1</td></tr><tr><th>modelId</th><td>amazon.titan-text-express-v1</td></tr><tr><th>modelName</th><td>Titan Text G1 - Express</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1:2:8k</td></tr><tr><th>modelId</th><td>amazon.titan-embed-text-v1:2:8k</td></tr><tr><th>modelName</th><td>Titan Embeddings G1 - Text</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>EMBEDDING</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1</td></tr><tr><th>modelId</th><td>amazon.titan-embed-text-v1</td></tr><tr><th>modelName</th><td>Titan Embeddings G1 - Text</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>EMBEDDING</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-image-v1:0</td></tr><tr><th>modelId</th><td>amazon.titan-embed-image-v1:0</td></tr><tr><th>modelName</th><td>Titan Multimodal Embeddings G1</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>EMBEDDING</li></ul></td></tr><tr><th>customizationsSupported</th><td><ul><li>FINE_TUNING</li></ul></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-image-v1</td></tr><tr><th>modelId</th><td>amazon.titan-embed-image-v1</td></tr><tr><th>modelName</th><td>Titan Multimodal Embeddings G1</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>EMBEDDING</li></ul></td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-image-generator-v1:0</td></tr><tr><th>modelId</th><td>amazon.titan-image-generator-v1:0</td></tr><tr><th>modelName</th><td>Titan Image Generator G1</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>IMAGE</li></ul></td></tr><tr><th>customizationsSupported</th><td><ul><li>FINE_TUNING</li></ul></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-image-generator-v1</td></tr><tr><th>modelId</th><td>amazon.titan-image-generator-v1</td></tr><tr><th>modelName</th><td>Titan Image Generator G1</td></tr><tr><th>providerName</th><td>Amazon</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>IMAGE</li></ul></td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl</td></tr><tr><th>modelId</th><td>stability.stable-diffusion-xl</td></tr><tr><th>modelName</th><td>SDXL 0.8</td></tr><tr><th>providerName</th><td>Stability AI</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>IMAGE</li></ul></td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>LEGACY</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v0</td></tr><tr><th>modelId</th><td>stability.stable-diffusion-xl-v0</td></tr><tr><th>modelName</th><td>SDXL 0.8</td></tr><tr><th>providerName</th><td>Stability AI</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>IMAGE</li></ul></td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>LEGACY</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v1:0</td></tr><tr><th>modelId</th><td>stability.stable-diffusion-xl-v1:0</td></tr><tr><th>modelName</th><td>SDXL 1.0</td></tr><tr><th>providerName</th><td>Stability AI</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>IMAGE</li></ul></td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v1</td></tr><tr><th>modelId</th><td>stability.stable-diffusion-xl-v1</td></tr><tr><th>modelName</th><td>SDXL 1.0</td></tr><tr><th>providerName</th><td>Stability AI</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>IMAGE</li></ul></td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-grande-instruct</td></tr><tr><th>modelId</th><td>ai21.j2-grande-instruct</td></tr><tr><th>modelName</th><td>J2 Grande Instruct</td></tr><tr><th>providerName</th><td>AI21 Labs</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-jumbo-instruct</td></tr><tr><th>modelId</th><td>ai21.j2-jumbo-instruct</td></tr><tr><th>modelName</th><td>J2 Jumbo Instruct</td></tr><tr><th>providerName</th><td>AI21 Labs</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-mid</td></tr><tr><th>modelId</th><td>ai21.j2-mid</td></tr><tr><th>modelName</th><td>Jurassic-2 Mid</td></tr><tr><th>providerName</th><td>AI21 Labs</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-mid-v1</td></tr><tr><th>modelId</th><td>ai21.j2-mid-v1</td></tr><tr><th>modelName</th><td>Jurassic-2 Mid</td></tr><tr><th>providerName</th><td>AI21 Labs</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-ultra</td></tr><tr><th>modelId</th><td>ai21.j2-ultra</td></tr><tr><th>modelName</th><td>Jurassic-2 Ultra</td></tr><tr><th>providerName</th><td>AI21 Labs</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-ultra-v1</td></tr><tr><th>modelId</th><td>ai21.j2-ultra-v1</td></tr><tr><th>modelName</th><td>Jurassic-2 Ultra</td></tr><tr><th>providerName</th><td>AI21 Labs</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-instant-v1:2:100k</td></tr><tr><th>modelId</th><td>anthropic.claude-instant-v1:2:100k</td></tr><tr><th>modelName</th><td>Claude Instant</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-instant-v1</td></tr><tr><th>modelId</th><td>anthropic.claude-instant-v1</td></tr><tr><th>modelName</th><td>Claude Instant</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:0:18k</td></tr><tr><th>modelId</th><td>anthropic.claude-v2:0:18k</td></tr><tr><th>modelName</th><td>Claude</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:0:100k</td></tr><tr><th>modelId</th><td>anthropic.claude-v2:0:100k</td></tr><tr><th>modelName</th><td>Claude</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1:18k</td></tr><tr><th>modelId</th><td>anthropic.claude-v2:1:18k</td></tr><tr><th>modelName</th><td>Claude</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1:200k</td></tr><tr><th>modelId</th><td>anthropic.claude-v2:1:200k</td></tr><tr><th>modelName</th><td>Claude</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1</td></tr><tr><th>modelId</th><td>anthropic.claude-v2:1</td></tr><tr><th>modelName</th><td>Claude</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2</td></tr><tr><th>modelId</th><td>anthropic.claude-v2</td></tr><tr><th>modelName</th><td>Claude</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:28k</td></tr><tr><th>modelId</th><td>anthropic.claude-3-sonnet-20240229-v1:0:28k</td></tr><tr><th>modelName</th><td>Claude 3 Sonnet</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:200k</td></tr><tr><th>modelId</th><td>anthropic.claude-3-sonnet-20240229-v1:0:200k</td></tr><tr><th>modelName</th><td>Claude 3 Sonnet</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0</td></tr><tr><th>modelId</th><td>anthropic.claude-3-sonnet-20240229-v1:0</td></tr><tr><th>modelName</th><td>Claude 3 Sonnet</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:48k</td></tr><tr><th>modelId</th><td>anthropic.claude-3-haiku-20240307-v1:0:48k</td></tr><tr><th>modelName</th><td>Claude 3 Haiku</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:200k</td></tr><tr><th>modelId</th><td>anthropic.claude-3-haiku-20240307-v1:0:200k</td></tr><tr><th>modelName</th><td>Claude 3 Haiku</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0</td></tr><tr><th>modelId</th><td>anthropic.claude-3-haiku-20240307-v1:0</td></tr><tr><th>modelName</th><td>Claude 3 Haiku</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-opus-20240229-v1:0</td></tr><tr><th>modelId</th><td>anthropic.claude-3-opus-20240229-v1:0</td></tr><tr><th>modelName</th><td>Claude 3 Opus</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/cohere.command-text-v14:7:4k</td></tr><tr><th>modelId</th><td>cohere.command-text-v14:7:4k</td></tr><tr><th>modelName</th><td>Command</td></tr><tr><th>providerName</th><td>Cohere</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td><ul><li>FINE_TUNING</li></ul></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/cohere.command-text-v14</td></tr><tr><th>modelId</th><td>cohere.command-text-v14</td></tr><tr><th>modelName</th><td>Command</td></tr><tr><th>providerName</th><td>Cohere</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/cohere.command-light-text-v14:7:4k</td></tr><tr><th>modelId</th><td>cohere.command-light-text-v14:7:4k</td></tr><tr><th>modelName</th><td>Command Light</td></tr><tr><th>providerName</th><td>Cohere</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td><ul><li>FINE_TUNING</li></ul></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/cohere.command-light-text-v14</td></tr><tr><th>modelId</th><td>cohere.command-light-text-v14</td></tr><tr><th>modelName</th><td>Command Light</td></tr><tr><th>providerName</th><td>Cohere</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-english-v3:0:512</td></tr><tr><th>modelId</th><td>cohere.embed-english-v3:0:512</td></tr><tr><th>modelName</th><td>Embed English</td></tr><tr><th>providerName</th><td>Cohere</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>EMBEDDING</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-english-v3</td></tr><tr><th>modelId</th><td>cohere.embed-english-v3</td></tr><tr><th>modelName</th><td>Embed English</td></tr><tr><th>providerName</th><td>Cohere</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>EMBEDDING</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-multilingual-v3:0:512</td></tr><tr><th>modelId</th><td>cohere.embed-multilingual-v3:0:512</td></tr><tr><th>modelName</th><td>Embed Multilingual</td></tr><tr><th>providerName</th><td>Cohere</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>EMBEDDING</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-multilingual-v3</td></tr><tr><th>modelId</th><td>cohere.embed-multilingual-v3</td></tr><tr><th>modelName</th><td>Embed Multilingual</td></tr><tr><th>providerName</th><td>Cohere</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>EMBEDDING</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>False</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-chat-v1:0:4k</td></tr><tr><th>modelId</th><td>meta.llama2-13b-chat-v1:0:4k</td></tr><tr><th>modelName</th><td>Llama 2 Chat 13B</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>PROVISIONED</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-chat-v1</td></tr><tr><th>modelId</th><td>meta.llama2-13b-chat-v1</td></tr><tr><th>modelName</th><td>Llama 2 Chat 13B</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-chat-v1:0:4k</td></tr><tr><th>modelId</th><td>meta.llama2-70b-chat-v1:0:4k</td></tr><tr><th>modelName</th><td>Llama 2 Chat 70B</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-chat-v1</td></tr><tr><th>modelId</th><td>meta.llama2-70b-chat-v1</td></tr><tr><th>modelName</th><td>Llama 2 Chat 70B</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-v1:0:4k</td></tr><tr><th>modelId</th><td>meta.llama2-13b-v1:0:4k</td></tr><tr><th>modelName</th><td>Llama 2 13B</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td><ul><li>FINE_TUNING</li></ul></td></tr><tr><th>inferenceTypesSupported</th><td></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-v1</td></tr><tr><th>modelId</th><td>meta.llama2-13b-v1</td></tr><tr><th>modelName</th><td>Llama 2 13B</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-v1:0:4k</td></tr><tr><th>modelId</th><td>meta.llama2-70b-v1:0:4k</td></tr><tr><th>modelName</th><td>Llama 2 70B</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td><ul><li>FINE_TUNING</li></ul></td></tr><tr><th>inferenceTypesSupported</th><td></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-v1</td></tr><tr><th>modelId</th><td>meta.llama2-70b-v1</td></tr><tr><th>modelName</th><td>Llama 2 70B</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-8b-instruct-v1:0</td></tr><tr><th>modelId</th><td>meta.llama3-8b-instruct-v1:0</td></tr><tr><th>modelName</th><td>Llama 3 8B Instruct</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-70b-instruct-v1:0</td></tr><tr><th>modelId</th><td>meta.llama3-70b-instruct-v1:0</td></tr><tr><th>modelName</th><td>Llama 3 70B Instruct</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/mistral.mistral-7b-instruct-v0:2</td></tr><tr><th>modelId</th><td>mistral.mistral-7b-instruct-v0:2</td></tr><tr><th>modelName</th><td>Mistral 7B Instruct</td></tr><tr><th>providerName</th><td>Mistral AI</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/mistral.mixtral-8x7b-instruct-v0:1</td></tr><tr><th>modelId</th><td>mistral.mixtral-8x7b-instruct-v0:1</td></tr><tr><th>modelName</th><td>Mixtral 8x7B Instruct</td></tr><tr><th>providerName</th><td>Mistral AI</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li><li><table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/mistral.mistral-large-2402-v1:0</td></tr><tr><th>modelId</th><td>mistral.mistral-large-2402-v1:0</td></tr><tr><th>modelName</th><td>Mistral Large</td></tr><tr><th>providerName</th><td>Mistral AI</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_string = json.dumps(bedrock_client.list_foundation_models()[\"modelSummaries\"])\n",
    "data = json.loads(model_string)\n",
    "display(HTML(json2html.convert(json = data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e0f85-4479-4d82-b8e5-1b47a4fd9a04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Claude 3 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86021ece-b38a-4382-83fe-ffd377c2b873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0</td></tr><tr><th>modelId</th><td>anthropic.claude-3-sonnet-20240229-v1:0</td></tr><tr><th>modelName</th><td>Claude 3 Sonnet</td></tr><tr><th>providerName</th><td>Anthropic</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li><li>IMAGE</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "claude_info=json.dumps(bedrock_client.get_foundation_model(modelIdentifier='anthropic.claude-3-sonnet-20240229-v1:0')['modelDetails'])\n",
    "data = json.loads(claude_info)\n",
    "display(HTML(json2html.convert(json = data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64267913-9e90-40d9-9511-814e946e6a10",
   "metadata": {},
   "source": [
    "### Llama 2 13B chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afdd5dc0-182b-4576-8b80-e45e1defa9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\"><tr><th>modelArn</th><td>arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-chat-v1</td></tr><tr><th>modelId</th><td>meta.llama2-13b-chat-v1</td></tr><tr><th>modelName</th><td>Llama 2 Chat 13B</td></tr><tr><th>providerName</th><td>Meta</td></tr><tr><th>inputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>outputModalities</th><td><ul><li>TEXT</li></ul></td></tr><tr><th>responseStreamingSupported</th><td>True</td></tr><tr><th>customizationsSupported</th><td></td></tr><tr><th>inferenceTypesSupported</th><td><ul><li>ON_DEMAND</li></ul></td></tr><tr><th>modelLifecycle</th><td><table border=\"1\"><tr><th>status</th><td>ACTIVE</td></tr></table></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama2_info=json.dumps(bedrock_client.get_foundation_model(modelIdentifier='meta.llama2-13b-chat-v1')['modelDetails'])\n",
    "data = json.loads(llama2_info)\n",
    "display(HTML(json2html.convert(json = data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e47ea28-ff62-430a-829a-01ed4feaf1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `Bedrock` was deprecated in LangChain 0.0.34 and will be removed in 0.3. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import BedrockLLM`.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `BedrockChat` was deprecated in LangChain 0.0.34 and will be removed in 0.3. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import ChatBedrock`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Bedrock\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "\n",
    "llama2_llm = Bedrock(\n",
    "    model_id=\"meta.llama2-13b-chat-v1\", \n",
    "    client=bedrock_runtime_client,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "claude3_llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", \n",
    "    client=bedrock_runtime_client,\n",
    "    verbose=True,\n",
    "    model_kwargs={\"temperature\": 0.0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2c54e-946f-484c-9f7a-2a9b72cc072b",
   "metadata": {},
   "source": [
    "## Interfacing with a database via Langchain SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d54f4-bd4f-45d9-a5aa-6c85314ebc78",
   "metadata": {},
   "source": [
    "In this section you will build the foudational element for a tool you will create later, which will take a natural language query and return a response from the in-memory SQLite database.\n",
    "\n",
    "You'll accomplish this by creating a Langchain SQLDatabase chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60f00101-10b9-4f01-aae2-f7d05210552c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain_experimental.sql import SQLDatabaseSequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342fb6e2-5aa8-46bd-af5a-4e45fb50780e",
   "metadata": {},
   "source": [
    "Below you can see an example prompt for your SQLDatabaseChain.\n",
    "\n",
    "It accepts:\n",
    "- `dialect`: parameter for the dialect of the target database\n",
    "- `table_info`: parameter that will autogenerate a sample of the schemas and first few rows of data from the tables\n",
    "- `input`: parameter for the input NLP query to generate SQL from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e86ea3cc-2386-4723-96ca-96a49312162e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"\n",
    "You are a {dialect} expert. Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per {dialect}. You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\\\") to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use date('now') function to get the current date, if the question involves \\\"today\\\".\n",
    "\n",
    "Use the following format:\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use the following tables enclosed in <tables> and </tables> tags:\n",
    "<tables>\n",
    "{table_info}\n",
    "</tables>\n",
    "\n",
    "Once you have a SQLResult, use it to formulate a natural language answer. <example>Answer: The policy id is 12345.</example>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"dialect\", \"top_k\"], template=_DEFAULT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4961fd-6d9d-4d73-a624-e55300b4cc7e",
   "metadata": {},
   "source": [
    "Create a SQLDatabase object from the DB engine created earlier for the SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78da1902-468d-46db-8612-07a1ddf8cfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = SQLDatabase(engine=sqllite_engine, include_tables=[\"policies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201ea63-c676-4627-a556-aae8cee0ec13",
   "metadata": {},
   "source": [
    "You can test your SQLDatabaseChain with a sample NLP query that will be the foundation of a later example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a26699b3-7464-4db8-bc64-343b7914eccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_chain = SQLDatabaseChain.from_llm(llm=llama2_llm, db=db, verbose=True, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b93cf-59f1-4fb0-8606-26d0ce5ebeff",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can test your SQLDatabaseChain with a sample NLP query that will be the foundation of a later example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2bb5911-51be-4e1e-88f2-97faee7df2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "What is Jewel Ingram's policy id?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT policy_id FROM policies WHERE first_name = 'Jewel' AND last_name = 'Ingram';\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(10157,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe policy id is 10157.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"What is Jewel Ingram's policy id?\",\n",
       " 'result': 'The policy id is 10157.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.debug=False\n",
    "sql_chain.invoke(input=\"What is Jewel Ingram's policy id?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe082a5-7f0f-48ee-83fa-b98fcfe7af24",
   "metadata": {},
   "source": [
    "After running the chain, you can see that it returns a policy id for the person in question. Sometimes you may see that the resulting query is not specfic enough (maybe only looking at `first_name` instead of both `first_name` and `last_name`), but this can be addressed by refining the prompt.\n",
    "\n",
    "However, it will suffice for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1998fa-aeee-40f0-b3b5-ba46a21d3ed1",
   "metadata": {},
   "source": [
    "---\n",
    "## Setting up Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec956d-eeaa-4963-8f1e-4dea99eac2cd",
   "metadata": {},
   "source": [
    "With the foundational components now in place, you will begin setting up the concept of tools. Tools are purpose built components to satisfy a particular need. They can look up data, run API commands, or even go perform searches on the internet to satisfy the needs of the reasoning chain of thought.\n",
    "\n",
    "As you proceed you'll see how to built an extensible toolbox that will attempt to automatically select the right tools based on the input query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda280d-008f-445d-a2bb-580701515c3b",
   "metadata": {},
   "source": [
    "### Create API Tool\n",
    "\n",
    "Next, we want our LLM to be able to carry out actions using an API that we will provide.  The API has been setup for you as a mock REST API in Amazon API Gateway. \n",
    "\n",
    "This API has PUT and DELETE methods, to accept requests to modify or cancel an insurance policy. The API takes  an input request and returns a successful response.  Our API isn't backed by any functional logic for simplicity in the workshop environment.  However, a real implementation would be backed by a fully functional API. \n",
    "\n",
    "To include the API in our ReAct workflow, we need to create an API tool.  For this, we are using LangChain's [StructuredTool](https://blog.langchain.dev/structured-tools/) which allows us to represent a function as a tool that an agent can easily interface with to perform actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a325208a-c993-40cb-8c09-90664c246e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_url='https://bkochd081f.execute-api.us-west-2.amazonaws.com/Mock-Test/policy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83f16fa0-1284-4d6c-966a-0d6a6d2c7c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "# A structured tool represents an action an agent can take. It wraps any function you provide to let an agent easily interface with it.\n",
    "    \n",
    "def cancel_policy_request(request: str): \n",
    "    \"\"\"Sends a DELETE request to the policy API with the provided policy id\"\"\"\n",
    "    url = request_url\n",
    "    policy_id = request.rsplit(None, 1)[-1]\n",
    "    result = requests.delete(url+\"?DELETE/\"+policy_id)\n",
    "    return f\"Successfully submitted cancel request for policy: {policy_id}, Status: {result.status_code} - {result.text}\"\n",
    "\n",
    "def update_policy_request(request: str):\n",
    "    \"\"\"Sends a PUT request to the policy API with the provided policy id and data to update\"\"\"\n",
    "    url = request_url\n",
    "    policy_id = request[request.find(\"policy\")+len(\"policy\"):].split()[0]\n",
    "    update_field = request[request.find(\"update the\")+len(\"update the\"):].split()[0]\n",
    "    update_data = request.rsplit(None, 1)[-1]\n",
    "    result = requests.put(url+\"?PUT/\"+policy_id, update_field+\"/\"+update_data)\n",
    "    return f\"Successfully submitted update for: {policy_id}, Update for: {update_field}, {update_data}, Status: {result.status_code} - {result.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1f69aef-4c27-46fd-8029-1d6b804c0eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_policy_request = StructuredTool.from_function(update_policy_request)\n",
    "cancel_policy_request = StructuredTool.from_function(cancel_policy_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d009d62-1f9c-4e4e-ac4a-6014c7271aff",
   "metadata": {},
   "source": [
    "Test the newly created tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "575cf842-d513-4abb-9f7e-d26e31b6fbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Successfully submitted cancel request for policy: 54621, Status: 200 - {\"statusCode\": 200 }'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancel_policy_request.run('the policy id is 54621')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef1760c7-0787-4f61-af09-dddbf6da41ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Successfully submitted update for: 54621, Update for: phone, 333-321-5622, Status: 200 - {\"statusCode\": 200 }'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_policy_request.run('Submit a request to update the phone number in policy 54621 to 333-321-5622')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c4fae-853d-4c9b-a9fb-75e5cc69c3d4",
   "metadata": {},
   "source": [
    "Let's now take all of the tools that we've created and add them all to the list of tools that will be made available to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c31fc62d-4ba0-4dc0-9d06-f7b1d236bc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "from langchain.agents import load_tools\n",
    "from langchain.tools import Tool\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(llama2_llm, verbose=True)\n",
    "\n",
    "ALL_TOOLS = [\n",
    "        Tool(\n",
    "            name=\"calculator\",\n",
    "            func=llm_math.run,\n",
    "            description=\"Useful when you need to perform mathematical operations.\"\n",
    "        ),    \n",
    "        Tool(\n",
    "            name=\"insurance_policy_lookup\",\n",
    "            func=sql_chain.run,\n",
    "            description=\"Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id. Example: [What is the policy id for Jane Doe?, What is Cynthia Stone's policy?]\"\n",
    "            #description=\"Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"cancel_policy_request\",\n",
    "            func=cancel_policy_request.run,\n",
    "            description=\"Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned. Example: [Submit a request to cancel policy for policy id of 4321]\"\n",
    "            #description=\"Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"update_policy_request\",\n",
    "            func=update_policy_request.run,\n",
    "            description=\"Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.  Example: [Submit a request to update the phone number in policy 4321 to 455-255-5555]\"\n",
    "            #description=\"Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.\"\n",
    "        ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38bfe5-7313-4a4a-9d6d-104c8bb63284",
   "metadata": {},
   "source": [
    "## Dynamic Tool Selection\n",
    "\n",
    "To create a more dynamic way for the reasoning engine to select the right tool based on the input, we'll setup our dynamic tool selector which will be responsible selecting the right tool based in the input using an embedding model and vector store. \n",
    "\n",
    "![dynamictool](./images/DynamicTools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac8478-3d00-46d2-a377-cf11e7f23e41",
   "metadata": {},
   "source": [
    "An [embedding model](https://huggingface.co/blog/getting-started-with-embeddings) takes a text string and converts it into a numerical vector representation, which will allow you to store it in a vector database and query it based on semantic similarity.\n",
    "\n",
    "Here, you will host a local [MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) embedding model, which will generate a set of embeddings to help semantically search for the right tools in the example.\n",
    "\n",
    "**Note: Ignore any \"Error displaying widget: model not found\" errors you may see here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abde9a56-ac00-4c51-b24d-0a0611ea90d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82318eaf482645fd84ee79ed35adaa2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf9ca0ee5174453869a97a1dfb66c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90073cafb5a24af09717fcea57e08157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24aa2c28e7cc4298a4fd56654acce36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe6f93f4a984fbc9cf43cb0ae86a9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 16:25:13.189270: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4373f673305f4275846b0e5149a6f13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a648316a32b043e8b82e1f201b2c0818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124c423cdc1b45eba879f38614552e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d8a324e3b84099ac7d3c3d38ac4a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183bbeb5dc1f4ef2a75b3b3347ba4ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738f05a9561c4bc3bd79fe0c5c008f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e314c-e06f-4be2-a5ce-0981825e7e75",
   "metadata": {},
   "source": [
    "The vector store method of tool selection in this example uses [Meta's Facebook AI Similarity Search (FAISS)](https://github.com/facebookresearch/faiss) as a vector database. There are many other options available as well.\n",
    "\n",
    "First you will create a set of documents (one for each tool), that you will vectorize and use to determine best match for tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b97abf3-519e-42d2-98f6-91f9639e0c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Useful when you need to perform mathematical operations.', metadata={'index': 0}),\n",
       " Document(page_content=\"Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id. Example: [What is the policy id for Jane Doe?, What is Cynthia Stone's policy?]\", metadata={'index': 1}),\n",
       " Document(page_content=\"Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned. Example: [Submit a request to cancel policy for policy id of 4321]\", metadata={'index': 2}),\n",
       " Document(page_content=\"Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.  Example: [Submit a request to update the phone number in policy 4321 to 455-255-5555]\", metadata={'index': 3})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=t.description, metadata={\"index\": i})\n",
    "    for i, t in enumerate(ALL_TOOLS)\n",
    "]\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b0676-71a0-4f46-829c-104e629614c9",
   "metadata": {},
   "source": [
    "FAISS provides a `from_documents` method for automatically generating embeddings and storing them in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5a4a233-3daf-4c03-ae6d-65e361b20efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a119c3-ce84-4f85-ae38-484175465b5a",
   "metadata": {},
   "source": [
    "In this section you will define the `get_tools` method, which will embed the input query and find relevant documents that are semantically close. This will allow you to take large lists of tools and scope them down to the most relevant ones.\n",
    "\n",
    "You'll use this method to demonstrate tool scoping below and it will also be used in supplying a list of tools to your agent later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68be0fb1-3815-40d1-90b0-3766c4fa813f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "def get_tools(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\n",
    "\n",
    "def print_tools(tools_arr):\n",
    "    for tool in tools_arr:\n",
    "        print(f'{tool}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0a5ba-1650-4de6-8859-42358ceb16c6",
   "metadata": {},
   "source": [
    "Let's see if we can find the right tool for a given input..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3b3a3-c83b-4293-a3a3-58841dcb04fc",
   "metadata": {},
   "source": [
    "Here you get a list of tools for a mathematical input query. Notice that the `calculator` tool is listed first as its description most closely matches the request. \n",
    "\n",
    "You can also control how many results to return by modifying the `get_relevant_documents` request inside of `get_tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c156111-d6af-462b-b77c-9e705bf0431d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='calculator' description='Useful when you need to perform mathematical operations.' func=<bound method Chain.run of LLMMathChain(verbose=True, llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=Bedrock(client=<botocore.client.BedrockRuntime object at 0x7faf24655540>, model_id='meta.llama2-13b-chat-v1', verbose=True)))>\n",
      "\n",
      "\n",
      "name='cancel_policy_request' description=\"Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned. Example: [Submit a request to cancel policy for policy id of 4321]\" func=<bound method BaseTool.run of StructuredTool(name='cancel_policy_request', description='cancel_policy_request(request: str) - Sends a DELETE request to the policy API with the provided policy id', args_schema=<class 'pydantic.v1.main.cancel_policy_requestSchema'>, func=<function cancel_policy_request at 0x7faf1e711bd0>)>\n",
      "\n",
      "\n",
      "name='update_policy_request' description=\"Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.  Example: [Submit a request to update the phone number in policy 4321 to 455-255-5555]\" func=<bound method BaseTool.run of StructuredTool(name='update_policy_request', description='update_policy_request(request: str) - Sends a PUT request to the policy API with the provided policy id and data to update', args_schema=<class 'pydantic.v1.main.update_policy_requestSchema'>, func=<function update_policy_request at 0x7faf1e79ca60>)>\n",
      "\n",
      "\n",
      "name='insurance_policy_lookup' description=\"Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id. Example: [What is the policy id for Jane Doe?, What is Cynthia Stone's policy?]\" func=<bound method Chain.run of SQLDatabaseChain(verbose=True, llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['dialect', 'input', 'table_info', 'top_k'], template='\\nYou are a {dialect} expert. Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per {dialect}. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use date(\\'now\\') function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables enclosed in <tables> and </tables> tags:\\n<tables>\\n{table_info}\\n</tables>\\n\\nOnce you have a SQLResult, use it to formulate a natural language answer. <example>Answer: The policy id is 12345.</example>\\n\\nQuestion: {input}'), llm=Bedrock(client=<botocore.client.BedrockRuntime object at 0x7faf24655540>, model_id='meta.llama2-13b-chat-v1', verbose=True)), database=<langchain_community.utilities.sql_database.SQLDatabase object at 0x7faf1e6daef0>)>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tools(get_tools(\"What is 281728^2?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183f523-4576-4738-9184-715c07b603ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "Alternatively, the `insurance_policy_lookup` tool is a better match for a policy related inquiry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2750fc1-05f1-443f-8b7c-ca18d52a08b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='insurance_policy_lookup' description=\"Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id. Example: [What is the policy id for Jane Doe?, What is Cynthia Stone's policy?]\" func=<bound method Chain.run of SQLDatabaseChain(verbose=True, llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['dialect', 'input', 'table_info', 'top_k'], template='\\nYou are a {dialect} expert. Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per {dialect}. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use date(\\'now\\') function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables enclosed in <tables> and </tables> tags:\\n<tables>\\n{table_info}\\n</tables>\\n\\nOnce you have a SQLResult, use it to formulate a natural language answer. <example>Answer: The policy id is 12345.</example>\\n\\nQuestion: {input}'), llm=Bedrock(client=<botocore.client.BedrockRuntime object at 0x7faf24655540>, model_id='meta.llama2-13b-chat-v1', verbose=True)), database=<langchain_community.utilities.sql_database.SQLDatabase object at 0x7faf1e6daef0>)>\n",
      "\n",
      "\n",
      "name='update_policy_request' description=\"Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.  Example: [Submit a request to update the phone number in policy 4321 to 455-255-5555]\" func=<bound method BaseTool.run of StructuredTool(name='update_policy_request', description='update_policy_request(request: str) - Sends a PUT request to the policy API with the provided policy id and data to update', args_schema=<class 'pydantic.v1.main.update_policy_requestSchema'>, func=<function update_policy_request at 0x7faf1e79ca60>)>\n",
      "\n",
      "\n",
      "name='cancel_policy_request' description=\"Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned. Example: [Submit a request to cancel policy for policy id of 4321]\" func=<bound method BaseTool.run of StructuredTool(name='cancel_policy_request', description='cancel_policy_request(request: str) - Sends a DELETE request to the policy API with the provided policy id', args_schema=<class 'pydantic.v1.main.cancel_policy_requestSchema'>, func=<function cancel_policy_request at 0x7faf1e711bd0>)>\n",
      "\n",
      "\n",
      "name='calculator' description='Useful when you need to perform mathematical operations.' func=<bound method Chain.run of LLMMathChain(verbose=True, llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=Bedrock(client=<botocore.client.BedrockRuntime object at 0x7faf24655540>, model_id='meta.llama2-13b-chat-v1', verbose=True)))>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tools(get_tools(\"What is the policy id for Bob Jones?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef5887-2059-4c23-912c-7556b9836cfe",
   "metadata": {},
   "source": [
    "While the `cancel_policy_request` tool is a better match for a policy cancellation request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed3b9999-0f8f-4cb3-88d6-f21dca18835c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='cancel_policy_request' description=\"Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned. Example: [Submit a request to cancel policy for policy id of 4321]\" func=<bound method BaseTool.run of StructuredTool(name='cancel_policy_request', description='cancel_policy_request(request: str) - Sends a DELETE request to the policy API with the provided policy id', args_schema=<class 'pydantic.v1.main.cancel_policy_requestSchema'>, func=<function cancel_policy_request at 0x7faf1e711bd0>)>\n",
      "\n",
      "\n",
      "name='update_policy_request' description=\"Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.  Example: [Submit a request to update the phone number in policy 4321 to 455-255-5555]\" func=<bound method BaseTool.run of StructuredTool(name='update_policy_request', description='update_policy_request(request: str) - Sends a PUT request to the policy API with the provided policy id and data to update', args_schema=<class 'pydantic.v1.main.update_policy_requestSchema'>, func=<function update_policy_request at 0x7faf1e79ca60>)>\n",
      "\n",
      "\n",
      "name='insurance_policy_lookup' description=\"Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id. Example: [What is the policy id for Jane Doe?, What is Cynthia Stone's policy?]\" func=<bound method Chain.run of SQLDatabaseChain(verbose=True, llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['dialect', 'input', 'table_info', 'top_k'], template='\\nYou are a {dialect} expert. Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per {dialect}. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use date(\\'now\\') function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables enclosed in <tables> and </tables> tags:\\n<tables>\\n{table_info}\\n</tables>\\n\\nOnce you have a SQLResult, use it to formulate a natural language answer. <example>Answer: The policy id is 12345.</example>\\n\\nQuestion: {input}'), llm=Bedrock(client=<botocore.client.BedrockRuntime object at 0x7faf24655540>, model_id='meta.llama2-13b-chat-v1', verbose=True)), database=<langchain_community.utilities.sql_database.SQLDatabase object at 0x7faf1e6daef0>)>\n",
      "\n",
      "\n",
      "name='calculator' description='Useful when you need to perform mathematical operations.' func=<bound method Chain.run of LLMMathChain(verbose=True, llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=Bedrock(client=<botocore.client.BedrockRuntime object at 0x7faf24655540>, model_id='meta.llama2-13b-chat-v1', verbose=True)))>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tools(get_tools(\"Cancel the policy for policy id 4325\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce42cbc-52e4-40f5-b634-5c1bca0502ac",
   "metadata": {},
   "source": [
    "# Agent Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f992b52-3680-43fd-ae1e-e6e87e51a826",
   "metadata": {},
   "source": [
    "With your models ready to go, and your toolbox configured, you're ready to setup your ReAct agent.\n",
    "\n",
    "The agent will take in a ReAct style prompt along with the list of tools to build out the chain of thought reasoning and resulting actions.\n",
    "\n",
    "The Langchain agent initialization takes 3 prompt keyword arguments to customize the agent's prompt:\n",
    "- `prefix`: the beginning of the prompt\n",
    "- \\<list of tools and their descriptions is automatically injected here\\>\n",
    "- `format_instructions`: how to format the intermediary and final responses, this is important in dictating how the chain of thought is created and processed.\n",
    "- `suffix`: The invocation of the agent. This contains the initial input request as well as the conversational chain to determine what the agent has already done and seen.\n",
    "\n",
    "Some template parameters you see below are:\n",
    "- `{tool_names}`: this is a list of the tools that are available to the agent, in name only. This helps direct the agent into what to supply as actions during reasoning. In this case the agent will do this step for you, but in others you may need to extract the names yourself.\n",
    "- `{agent_scratchpad}`: this is important as it documents the activities and obserations that the agent has had throughout its reasoning. Without this, you will normally see the agent endlessly loop.\n",
    "- `{input}`: The input request from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0507421e-b2e8-4749-b1b3-d46632008338",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_AGENT_PROMPT_TEMPLATE = \"\"\"Human: You are an agent tasked with helping look up and modify insurance claims.\n",
    "\n",
    "Given an input request, take a step-by-step approach to find an insurance policy and modify its status.\n",
    "Only use the tools provided. Do NOT assume any information that hasn't been included in the coversation history.\n",
    "When you have completed the task, end your chain of thought and provide a final response to the user.\n",
    "\n",
    "You have access to the following tools:\n",
    "<tools>\n",
    "{tools}\n",
    "</tools>\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "\n",
    "\n",
    "When you have a response to say to the User, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:{agent_scratchpad}\n",
    "Original input: {input}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "AGENT_PROMPT = PromptTemplate(\n",
    "    input_variables=['tools', 'tool_names', 'agent_scratchpad', 'input'], template=_DEFAULT_AGENT_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5abd42ac-7ab1-4450-8ea7-9cb733aa1c09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_tool_calling_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147d016-ec63-4c82-ab04-4386fcc86b11",
   "metadata": {},
   "source": [
    "This is the user request that the agent will process using the provided tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfb3152d-392d-4c44-9588-1d78c2858b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4fd87-f421-46a7-8174-3f6357c3a4c4",
   "metadata": {},
   "source": [
    "Here you will set up your agent.\n",
    "\n",
    "initialize_agent has the following configuration:\n",
    "- `agent`: using the [ZERO_SHOT_REACT agent](https://python.langchain.com/docs/modules/agents/agent_types/react#using-zeroshotreactagent)\n",
    "- `agent_kwargs`: variable contains the object with all of the customized prompt information.\n",
    "- `tools`: list of tools from the vector store, dynamically generated from the input query\n",
    "- `llm`: the agent LLM (in this case Claude V2 from Amazon Bedrock)\n",
    "- `verbose`: show all the details for learning purposes\n",
    "- `return_intermediate_steps`: important for retaining all of the thoughts/actions/observations in the agent_scratchpad\n",
    "- `max_iterations`: VERY IMPORTANT for ensuring that your agent doesn't endlessly loop and run away. Prevents the agent from hammering on the associated LLM.\n",
    "- `handle_parsing_errors`: used for dealing with issues parsing the output of a given step, not necessary but catches edge cases in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "687a3b2b-0920-4f4b-8699-761487b25a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = get_tools(query)\n",
    "\n",
    "agent = create_react_agent(llm=claude3_llm, tools=tools, prompt=AGENT_PROMPT)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294923e8-8957-466c-a436-aa01f1bf6717",
   "metadata": {},
   "source": [
    "Finally you can invoke your agent with the input query!\n",
    "\n",
    "For the first run, we will run with debug = False which will show condensed output from the agent. Following this run, we'll run the same query with debug = True if you want to dive deeper into the details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f2415d1-bbc9-48b7-b3d9-34d7f988cb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "What is the policy id for Charles Wise?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT policy_id FROM policies WHERE first_name = 'Charles' AND last_name = 'Wise';\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(54621,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe policy id for Charles Wise is 54621.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622',\n",
       " 'output': \"I have successfully updated the phone number for Charles Wise's insurance policy 54621 to 333-321-5622.\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.debug = False\n",
    "\n",
    "agent_executor.invoke({\"input\":query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aead9f-8954-4da1-b439-3056af299598",
   "metadata": {},
   "source": [
    "Let's try to rerun it with debug set to 'true' where you can dive deep into the details on the Thought > Observation > Action workflow. The output will be verbose, but if you follow through you will see the chain start/end and invoke tools to get the information and take action until it reaches a conclusion or hits the `max_iterations`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b7ecdb4-a575-42b6-b446-b42e7979a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] [21ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] [51ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622\",\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622\",\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:BedrockChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Human: You are an agent tasked with helping look up and modify insurance claims.\\n\\nGiven an input request, take a step-by-step approach to find an insurance policy and modify its status.\\nOnly use the tools provided. Do NOT assume any information that hasn't been included in the coversation history.\\nWhen you have completed the task, end your chain of thought and provide a final response to the user.\\n\\nYou have access to the following tools:\\n<tools>\\nupdate_policy_request: Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.  Example: [Submit a request to update the phone number in policy 4321 to 455-255-5555]\\ncancel_policy_request: Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned. Example: [Submit a request to cancel policy for policy id of 4321]\\ninsurance_policy_lookup: Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id. Example: [What is the policy id for Jane Doe?, What is Cynthia Stone's policy?]\\ncalculator: Useful when you need to perform mathematical operations.\\n</tools>\\n\\nTo use a tool, please use the following format:\\n\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [update_policy_request, cancel_policy_request, insurance_policy_lookup, calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n\\n\\nWhen you have a response to say to the User, or if you do not need to use a tool, you MUST use the format:\\n\\nThought: Do I need to use a tool? No\\nFinal Answer: [your response here]\\n\\n\\nBegin!\\n\\nPrevious conversation history:\\nOriginal input: Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622\\n\\nAssistant:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:BedrockChat] [954ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Thought: Do I need to use a tool? Yes\\nAction: insurance_policy_lookup\\nAction Input: What is the policy id for Charles Wise?\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Thought: Do I need to use a tool? Yes\\nAction: insurance_policy_lookup\\nAction Input: What is the policy id for Charles Wise?\",\n",
      "            \"example\": false,\n",
      "            \"additional_kwargs\": {},\n",
      "            \"tool_call_chunks\": [],\n",
      "            \"response_metadata\": {},\n",
      "            \"id\": \"run-d400b56a-9571-4616-a1e1-fc301976ef90\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:ReActSingleInputOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:ReActSingleInputOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] [1.05s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup] Entering Tool run with input:\n",
      "\u001b[0m\"What is the policy id for Charles Wise?\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What is the policy id for Charles Wise?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain > 11:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the policy id for Charles Wise?\\nSQLQuery:\",\n",
      "  \"top_k\": \"5\",\n",
      "  \"dialect\": \"sqlite\",\n",
      "  \"table_info\": \"\\nCREATE TABLE policies (\\n\\tpolicy_id INTEGER NOT NULL, \\n\\tfirst_name VARCHAR(50) NOT NULL, \\n\\tlast_name VARCHAR(50) NOT NULL, \\n\\tphone VARCHAR(15) NOT NULL, \\n\\tpolicy_type VARCHAR(25) NOT NULL, \\n\\tpolicy_date DATE NOT NULL, \\n\\tpolicy_value INTEGER NOT NULL, \\n\\tPRIMARY KEY (policy_id)\\n)\\n\\n/*\\n3 rows from policies table:\\npolicy_id\\tfirst_name\\tlast_name\\tphone\\tpolicy_type\\tpolicy_date\\tpolicy_value\\n10157\\tJewel\\tIngram\\t598-338-6133\\thome\\t2023-01-06\\t750000\\n17108\\tMargarito\\tRollins\\t348-321-5711\\tauto\\t2023-01-04\\t75000\\n17565\\tCharmaine\\tHopkins\\t206-566-6359\\thome\\t2023-01-02\\t135000\\n*/\",\n",
      "  \"stop\": [\n",
      "    \"\\nSQLResult:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain > 11:chain:LLMChain > 12:llm:Bedrock] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a sqlite expert. Given an input question, first create a syntactically correct sqlite query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per sqlite. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\\\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use date('now') function to get the current date, if the question involves \\\"today\\\".\\n\\nUse the following format:\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables enclosed in <tables> and </tables> tags:\\n<tables>\\n\\nCREATE TABLE policies (\\n\\tpolicy_id INTEGER NOT NULL, \\n\\tfirst_name VARCHAR(50) NOT NULL, \\n\\tlast_name VARCHAR(50) NOT NULL, \\n\\tphone VARCHAR(15) NOT NULL, \\n\\tpolicy_type VARCHAR(25) NOT NULL, \\n\\tpolicy_date DATE NOT NULL, \\n\\tpolicy_value INTEGER NOT NULL, \\n\\tPRIMARY KEY (policy_id)\\n)\\n\\n/*\\n3 rows from policies table:\\npolicy_id\\tfirst_name\\tlast_name\\tphone\\tpolicy_type\\tpolicy_date\\tpolicy_value\\n10157\\tJewel\\tIngram\\t598-338-6133\\thome\\t2023-01-06\\t750000\\n17108\\tMargarito\\tRollins\\t348-321-5711\\tauto\\t2023-01-04\\t75000\\n17565\\tCharmaine\\tHopkins\\t206-566-6359\\thome\\t2023-01-02\\t135000\\n*/\\n</tables>\\n\\nOnce you have a SQLResult, use it to formulate a natural language answer. <example>Answer: The policy id is 12345.</example>\\n\\nQuestion: What is the policy id for Charles Wise?\\nSQLQuery:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain > 11:chain:LLMChain > 12:llm:Bedrock] [1.37s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" SELECT policy_id FROM policies WHERE first_name = 'Charles' AND last_name = 'Wise';\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain > 11:chain:LLMChain] [1.38s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \" SELECT policy_id FROM policies WHERE first_name = 'Charles' AND last_name = 'Wise';\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain > 13:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the policy id for Charles Wise?\\nSQLQuery:SELECT policy_id FROM policies WHERE first_name = 'Charles' AND last_name = 'Wise';\\nSQLResult: [(54621,)]\\nAnswer:\",\n",
      "  \"top_k\": \"5\",\n",
      "  \"dialect\": \"sqlite\",\n",
      "  \"table_info\": \"\\nCREATE TABLE policies (\\n\\tpolicy_id INTEGER NOT NULL, \\n\\tfirst_name VARCHAR(50) NOT NULL, \\n\\tlast_name VARCHAR(50) NOT NULL, \\n\\tphone VARCHAR(15) NOT NULL, \\n\\tpolicy_type VARCHAR(25) NOT NULL, \\n\\tpolicy_date DATE NOT NULL, \\n\\tpolicy_value INTEGER NOT NULL, \\n\\tPRIMARY KEY (policy_id)\\n)\\n\\n/*\\n3 rows from policies table:\\npolicy_id\\tfirst_name\\tlast_name\\tphone\\tpolicy_type\\tpolicy_date\\tpolicy_value\\n10157\\tJewel\\tIngram\\t598-338-6133\\thome\\t2023-01-06\\t750000\\n17108\\tMargarito\\tRollins\\t348-321-5711\\tauto\\t2023-01-04\\t75000\\n17565\\tCharmaine\\tHopkins\\t206-566-6359\\thome\\t2023-01-02\\t135000\\n*/\",\n",
      "  \"stop\": [\n",
      "    \"\\nSQLResult:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain > 13:chain:LLMChain > 14:llm:Bedrock] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a sqlite expert. Given an input question, first create a syntactically correct sqlite query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per sqlite. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\\\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use date('now') function to get the current date, if the question involves \\\"today\\\".\\n\\nUse the following format:\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables enclosed in <tables> and </tables> tags:\\n<tables>\\n\\nCREATE TABLE policies (\\n\\tpolicy_id INTEGER NOT NULL, \\n\\tfirst_name VARCHAR(50) NOT NULL, \\n\\tlast_name VARCHAR(50) NOT NULL, \\n\\tphone VARCHAR(15) NOT NULL, \\n\\tpolicy_type VARCHAR(25) NOT NULL, \\n\\tpolicy_date DATE NOT NULL, \\n\\tpolicy_value INTEGER NOT NULL, \\n\\tPRIMARY KEY (policy_id)\\n)\\n\\n/*\\n3 rows from policies table:\\npolicy_id\\tfirst_name\\tlast_name\\tphone\\tpolicy_type\\tpolicy_date\\tpolicy_value\\n10157\\tJewel\\tIngram\\t598-338-6133\\thome\\t2023-01-06\\t750000\\n17108\\tMargarito\\tRollins\\t348-321-5711\\tauto\\t2023-01-04\\t75000\\n17565\\tCharmaine\\tHopkins\\t206-566-6359\\thome\\t2023-01-02\\t135000\\n*/\\n</tables>\\n\\nOnce you have a SQLResult, use it to formulate a natural language answer. <example>Answer: The policy id is 12345.</example>\\n\\nQuestion: What is the policy id for Charles Wise?\\nSQLQuery:SELECT policy_id FROM policies WHERE first_name = 'Charles' AND last_name = 'Wise';\\nSQLResult: [(54621,)]\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain > 13:chain:LLMChain > 14:llm:Bedrock] [673ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" The policy id for Charles Wise is 54621.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain > 13:chain:LLMChain] [677ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \" The policy id for Charles Wise is 54621.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup > 10:chain:SQLDatabaseChain] [2.06s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"The policy id for Charles Wise is 54621.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 9:tool:insurance_policy_lookup] [2.06s] Exiting Tool run with output:\n",
      "\u001b[0m\"The policy id for Charles Wise is 54621.\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 16:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 16:chain:RunnableAssign<agent_scratchpad> > 17:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 16:chain:RunnableAssign<agent_scratchpad> > 17:chain:RunnableParallel<agent_scratchpad> > 18:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 16:chain:RunnableAssign<agent_scratchpad> > 17:chain:RunnableParallel<agent_scratchpad> > 18:chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Thought: Do I need to use a tool? Yes\\nAction: insurance_policy_lookup\\nAction Input: What is the policy id for Charles Wise?\\nObservation: The policy id for Charles Wise is 54621.\\nThought: \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 16:chain:RunnableAssign<agent_scratchpad> > 17:chain:RunnableParallel<agent_scratchpad>] [7ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"agent_scratchpad\": \"Thought: Do I need to use a tool? Yes\\nAction: insurance_policy_lookup\\nAction Input: What is the policy id for Charles Wise?\\nObservation: The policy id for Charles Wise is 54621.\\nThought: \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 16:chain:RunnableAssign<agent_scratchpad>] [9ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 19:prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 19:prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 20:llm:BedrockChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Human: You are an agent tasked with helping look up and modify insurance claims.\\n\\nGiven an input request, take a step-by-step approach to find an insurance policy and modify its status.\\nOnly use the tools provided. Do NOT assume any information that hasn't been included in the coversation history.\\nWhen you have completed the task, end your chain of thought and provide a final response to the user.\\n\\nYou have access to the following tools:\\n<tools>\\nupdate_policy_request: Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.  Example: [Submit a request to update the phone number in policy 4321 to 455-255-5555]\\ncancel_policy_request: Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned. Example: [Submit a request to cancel policy for policy id of 4321]\\ninsurance_policy_lookup: Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id. Example: [What is the policy id for Jane Doe?, What is Cynthia Stone's policy?]\\ncalculator: Useful when you need to perform mathematical operations.\\n</tools>\\n\\nTo use a tool, please use the following format:\\n\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [update_policy_request, cancel_policy_request, insurance_policy_lookup, calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n\\n\\nWhen you have a response to say to the User, or if you do not need to use a tool, you MUST use the format:\\n\\nThought: Do I need to use a tool? No\\nFinal Answer: [your response here]\\n\\n\\nBegin!\\n\\nPrevious conversation history:Thought: Do I need to use a tool? Yes\\nAction: insurance_policy_lookup\\nAction Input: What is the policy id for Charles Wise?\\nObservation: The policy id for Charles Wise is 54621.\\nThought: \\nOriginal input: Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622\\n\\nAssistant:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 20:llm:BedrockChat] [1.08s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Thought: Do I need to use a tool? Yes\\nAction: update_policy_request\\nAction Input: Submit a request to update the phone number in policy 54621 to 333-321-5622\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Thought: Do I need to use a tool? Yes\\nAction: update_policy_request\\nAction Input: Submit a request to update the phone number in policy 54621 to 333-321-5622\",\n",
      "            \"example\": false,\n",
      "            \"additional_kwargs\": {},\n",
      "            \"tool_call_chunks\": [],\n",
      "            \"response_metadata\": {},\n",
      "            \"id\": \"run-14f6e1c1-ee30-407d-b022-bdff6a6501eb\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 21:parser:ReActSingleInputOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence > 21:parser:ReActSingleInputOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 15:chain:RunnableSequence] [1.10s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 22:tool:update_policy_request] Entering Tool run with input:\n",
      "\u001b[0m\"Submit a request to update the phone number in policy 54621 to 333-321-5622\"\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 22:tool:update_policy_request > 23:tool:update_policy_request] Entering Tool run with input:\n",
      "\u001b[0m\"Submit a request to update the phone number in policy 54621 to 333-321-5622\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 22:tool:update_policy_request > 23:tool:update_policy_request] [37ms] Exiting Tool run with output:\n",
      "\u001b[0m\"Successfully submitted update for: 54621, Update for: phone, 333-321-5622, Status: 200 - {\"statusCode\": 200 }\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 22:tool:update_policy_request] [38ms] Exiting Tool run with output:\n",
      "\u001b[0m\"Successfully submitted update for: 54621, Update for: phone, 333-321-5622, Status: 200 - {\"statusCode\": 200 }\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 25:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 25:chain:RunnableAssign<agent_scratchpad> > 26:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 25:chain:RunnableAssign<agent_scratchpad> > 26:chain:RunnableParallel<agent_scratchpad> > 27:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 25:chain:RunnableAssign<agent_scratchpad> > 26:chain:RunnableParallel<agent_scratchpad> > 27:chain:RunnableLambda] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Thought: Do I need to use a tool? Yes\\nAction: insurance_policy_lookup\\nAction Input: What is the policy id for Charles Wise?\\nObservation: The policy id for Charles Wise is 54621.\\nThought: Thought: Do I need to use a tool? Yes\\nAction: update_policy_request\\nAction Input: Submit a request to update the phone number in policy 54621 to 333-321-5622\\nObservation: Successfully submitted update for: 54621, Update for: phone, 333-321-5622, Status: 200 - {\\\"statusCode\\\": 200 }\\nThought: \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 25:chain:RunnableAssign<agent_scratchpad> > 26:chain:RunnableParallel<agent_scratchpad>] [8ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"agent_scratchpad\": \"Thought: Do I need to use a tool? Yes\\nAction: insurance_policy_lookup\\nAction Input: What is the policy id for Charles Wise?\\nObservation: The policy id for Charles Wise is 54621.\\nThought: Thought: Do I need to use a tool? Yes\\nAction: update_policy_request\\nAction Input: Submit a request to update the phone number in policy 54621 to 333-321-5622\\nObservation: Successfully submitted update for: 54621, Update for: phone, 333-321-5622, Status: 200 - {\\\"statusCode\\\": 200 }\\nThought: \"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 25:chain:RunnableAssign<agent_scratchpad>] [16ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 28:prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 28:prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 29:llm:BedrockChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Human: You are an agent tasked with helping look up and modify insurance claims.\\n\\nGiven an input request, take a step-by-step approach to find an insurance policy and modify its status.\\nOnly use the tools provided. Do NOT assume any information that hasn't been included in the coversation history.\\nWhen you have completed the task, end your chain of thought and provide a final response to the user.\\n\\nYou have access to the following tools:\\n<tools>\\nupdate_policy_request: Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.  Example: [Submit a request to update the phone number in policy 4321 to 455-255-5555]\\ncancel_policy_request: Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned. Example: [Submit a request to cancel policy for policy id of 4321]\\ninsurance_policy_lookup: Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id. Example: [What is the policy id for Jane Doe?, What is Cynthia Stone's policy?]\\ncalculator: Useful when you need to perform mathematical operations.\\n</tools>\\n\\nTo use a tool, please use the following format:\\n\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [update_policy_request, cancel_policy_request, insurance_policy_lookup, calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n\\n\\nWhen you have a response to say to the User, or if you do not need to use a tool, you MUST use the format:\\n\\nThought: Do I need to use a tool? No\\nFinal Answer: [your response here]\\n\\n\\nBegin!\\n\\nPrevious conversation history:Thought: Do I need to use a tool? Yes\\nAction: insurance_policy_lookup\\nAction Input: What is the policy id for Charles Wise?\\nObservation: The policy id for Charles Wise is 54621.\\nThought: Thought: Do I need to use a tool? Yes\\nAction: update_policy_request\\nAction Input: Submit a request to update the phone number in policy 54621 to 333-321-5622\\nObservation: Successfully submitted update for: 54621, Update for: phone, 333-321-5622, Status: 200 - {\\\"statusCode\\\": 200 }\\nThought: \\nOriginal input: Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622\\n\\nAssistant:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 29:llm:BedrockChat] [1.36s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Thought: Do I need to use a tool? No\\nFinal Answer: I have successfully updated the phone number for Charles Wise's insurance policy 54621 to 333-321-5622.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Thought: Do I need to use a tool? No\\nFinal Answer: I have successfully updated the phone number for Charles Wise's insurance policy 54621 to 333-321-5622.\",\n",
      "            \"example\": false,\n",
      "            \"additional_kwargs\": {},\n",
      "            \"tool_call_chunks\": [],\n",
      "            \"response_metadata\": {},\n",
      "            \"id\": \"run-6d5be0cb-bdd9-4c71-a15e-7ee84925bfd6\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 30:parser:ReActSingleInputOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence > 30:parser:ReActSingleInputOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 24:chain:RunnableSequence] [1.39s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [5.73s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"I have successfully updated the phone number for Charles Wise's insurance policy 54621 to 333-321-5622.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622',\n",
       " 'output': \"I have successfully updated the phone number for Charles Wise's insurance policy 54621 to 333-321-5622.\"}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.debug = True\n",
    "\n",
    "agent_executor.invoke({\"input\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af6965-c609-4117-a506-8f0320063a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
