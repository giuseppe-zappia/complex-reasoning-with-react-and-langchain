{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8769ba-5cdb-4e55-84b9-e3fa0f7e8cac",
   "metadata": {},
   "source": [
    "# Enabling Complex Reasoning and Action with ReAct, LLMs, and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fdda37-63cb-4a61-89ec-0b7900ae83d2",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to use multiple different techniques and models to build a ReAct based framework. ReAct is an approach to problem solving with large language models based on 2 main premises: Reasoning and Action. With ReAct, you combine reasoning, through chain-of-thought, with the ability to perform actions through a set of tools. This enables the model to (Re)ason through the input request to determine what steps need to be performed, and uses the available tools to perform (ACT)ions as part of a step-by-step resolution.\n",
    "\n",
    "More details on ReAct can be found in this research paper: [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629) and the [Google AI Blog](https://blog.research.google/2022/11/react-synergizing-reasoning-and-acting.html)\n",
    "\n",
    "![image.png](ReAct-Diagram.png)\n",
    "\n",
    "Image taken from Google AI Blog\n",
    "\n",
    "To demonstrate the potential of ReAct this notebook will focus on a use case involving and Insurance Bot.  For demonstration purposes, this Bot is designed to handle insurance policy requests and is provided a set of tools, including a SQLLite database and an insurance processing API, to accept requests on input, reason through the steps needed to carry out the request, and carry out the actions required.  \n",
    "\n",
    "![image.png](./images/Overview-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c9b99-de64-4829-ba40-079b58721faa",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a11660-abf0-44ff-b0d9-e91a36ca95f1",
   "metadata": {},
   "source": [
    "As part of this workshop you will run a local, quantized version of Meta's LLaMa-2-13B. In order do do this you will need llama-cpp-python, which requires a local compiler. This will install `g++` and set it as the default so that you can install llama-cpp-python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e030aa9a-3e86-47a9-a031-928e51ebfe34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install ipywidgets==8.1.2\n",
    "%pip install transformers[torch]==4.39.3\n",
    "%pip install sentence_transformers==2.6.1\n",
    "%pip install langchain==0.1.16\n",
    "%pip install faiss-cpu==1.8.0\n",
    "%pip install langchain-experimental==0.0.57\n",
    "%pip install sqlalchemy==2.0.29\n",
    "%pip install json2html==1.3.0\n",
    "%pip install numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fff23-2268-454e-bf32-31d3a6433166",
   "metadata": {},
   "source": [
    "---\n",
    "# ========>  IMPORTANT!! <============\n",
    "\n",
    "Please restart your notebook kernel by either clicking the refresh button in the notebook menu bar or going to Kernel > Restart Kernel .  You can then resume the next steps in your notebook starting with the cell below. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02fb72-f2e7-4a70-b62c-cd015548a667",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bedrock Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86abfdde-e12a-48d4-8fef-5c9cc8ac9a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "bedrock_client = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2263d-b376-4aee-a4b5-6962891d9b7b",
   "metadata": {},
   "source": [
    "# Workshop Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06870628-2f15-4da1-84fd-2464ac913a42",
   "metadata": {
    "tags": []
   },
   "source": [
    "Because we'll be using LangChain for this workshop, we'll need to import the library to start..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7229813-f772-4e85-80bb-9a060d1d857e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0e591-2f09-415f-8732-45cbbbe6c8a8",
   "metadata": {},
   "source": [
    "## Setup sample data using SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f981bd-9e1e-4300-88c7-b321c2ec8639",
   "metadata": {
    "tags": []
   },
   "source": [
    "As part of this workshop, you will see how to work with a variety of different tool types to retrieve and act on data.\n",
    "\n",
    "Here you will create an in-memory SQLite database to store some sample data for the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f39b8f-0aa9-4600-9bf2-553dcca45d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5440ac-1da0-4f5c-b321-b5c3102030f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Build out a table of insurance policies. This would typically be normalized, but for simplicity's sake of this example, the policies and users are all in 1 table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c276bfd9-e896-4574-826b-be13cbfd1340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Table, Date\n",
    "\n",
    "policies = Table(\n",
    "    \"policies\",\n",
    "    metadata_obj,\n",
    "    Column(\"policy_id\", Integer, primary_key=True),\n",
    "    Column(\"first_name\", String(50), nullable=False),\n",
    "    Column(\"last_name\", String(50), nullable=False),\n",
    "    Column(\"phone\", String(15), nullable=False),\n",
    "    Column(\"policy_type\", String(25), nullable=False),\n",
    "    Column(\"policy_date\", Date, nullable=False),\n",
    "    Column(\"policy_value\", Integer, nullable=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47fef6d-d9f1-4004-890d-f85533b39fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "sqllite_engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj.create_all(sqllite_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f154acc-50c3-4d02-9798-8a7c9304d8d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generate some some dummy data and insert it into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79677473-6862-4545-b774-efd8cb62f606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#policy data: policy_id, first_name, last_name, phone, policy_type, policy_date, policy_value\n",
    "\n",
    "policy_data = [\n",
    "    [48918, 'Ernest', 'Mcneil', '349-711-8757', 'home', datetime(2023, 1, 1), 250000],\n",
    "    [66958, 'Brian', 'Patel', '368-889-1742', 'auto', datetime(2023, 1, 2), 32000],\n",
    "    [21947, 'Bertram', 'Mcgee', '798-641-5925', 'home', datetime(2023, 1, 3), 550000],\n",
    "    [17108, 'Margarito', 'Rollins', '348-321-5711', 'auto', datetime(2023, 1, 4), 75000],\n",
    "    [98362, 'Miriam', 'Sutton', '361-863-4332', 'auto', datetime(2023, 1, 8), 21000],\n",
    "    [17565, 'Charmaine', 'Hopkins', '206-566-6359', 'home', datetime(2023, 1, 2), 135000],\n",
    "    [10157, 'Jewel', 'Ingram', '598-338-6133', 'home', datetime(2023, 1, 6), 750000],\n",
    "    [33372, 'Kaye', 'Underwood', '555-720-3848', 'home', datetime(2023, 1, 1), 235000],\n",
    "    [97143, 'Josiah', 'Vazquez', '211-391-1757', 'auto', datetime(2023, 1, 5), 17250],\n",
    "    [54621, 'Charles', 'Wise', '502-236-0425', 'home', datetime(2023, 1, 4), 1592000],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "517c127a-5bcf-4252-bbf6-9f7c993f7793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "\n",
    "def insert_policy_data(policy_data_arr):\n",
    "    stmt = insert(policies).values(\n",
    "    policy_id=policy_data_arr[0],\n",
    "    first_name=policy_data_arr[1],\n",
    "    last_name=policy_data_arr[2],\n",
    "    phone=policy_data_arr[3],\n",
    "    policy_type=policy_data_arr[4],\n",
    "    policy_date=policy_data_arr[5],\n",
    "    policy_value=policy_data_arr[6]\n",
    "    )\n",
    "\n",
    "    with sqllite_engine.begin() as conn:\n",
    "        conn.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e78b6e-209d-4cc9-884e-2b6f7a362e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for policy in policy_data:\n",
    "    print(policy)\n",
    "    insert_policy_data(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532aee6-46a0-4e6c-a9a7-475791759987",
   "metadata": {},
   "source": [
    "Quick query over the database to show all the rows, sorted by last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "215333c8-cdb0-4a08-88e2-ea4aa6e8184a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "stmt = select(policies).order_by(policies.c.last_name)\n",
    "print(f'{stmt}\\n')\n",
    "\n",
    "with sqllite_engine.connect() as conn:\n",
    "    for row in conn.execute(stmt):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568a15a-6a98-4c8f-87d4-7c25e1e0cbc3",
   "metadata": {},
   "source": [
    "# Configure Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3425bc-4444-4a90-8985-30a76ccaad71",
   "metadata": {},
   "source": [
    "To show diversity in approaches, we'll use two models from [Amazon Bedrock](https://aws.amazon.com/bedrock/) in this workshop.  \n",
    "\n",
    "   1. **LLaMa-2-13B-Chat** model will be used to generate SQL to query the database you just made  \n",
    "   2. **Anthropic's Claude V3 Sonnet** will be used as the LLM for the reasoning part of the ReAct approach in the form of a [Langchain Agent](https://python.langchain.com/docs/modules/agents/). This model will be responsible for accepting the request, breaking down the chain of thought reasoning, selecting tools, and formulate a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25d34fd9-5b8b-42b8-aad3-a490bf28a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json2html import *\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "150bed4b-cc6c-4b9d-b13c-a546f1bc09bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_string = json.dumps(bedrock_client.list_foundation_models()[\"modelSummaries\"])\n",
    "data = json.loads(model_string)\n",
    "display(HTML(json2html.convert(json = data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e0f85-4479-4d82-b8e5-1b47a4fd9a04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Claude 3 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86021ece-b38a-4382-83fe-ffd377c2b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_info=json.dumps(bedrock_client.get_foundation_model(modelIdentifier='anthropic.claude-3-sonnet-20240229-v1:0')['modelDetails'])\n",
    "data = json.loads(claude_info)\n",
    "display(HTML(json2html.convert(json = data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64267913-9e90-40d9-9511-814e946e6a10",
   "metadata": {},
   "source": [
    "### Llama 2 13B chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afdd5dc0-182b-4576-8b80-e45e1defa9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_info=json.dumps(bedrock_client.get_foundation_model(modelIdentifier='meta.llama2-13b-chat-v1')['modelDetails'])\n",
    "data = json.loads(llama2_info)\n",
    "display(HTML(json2html.convert(json = data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e47ea28-ff62-430a-829a-01ed4feaf1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Bedrock\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "\n",
    "llama2_llm = Bedrock(\n",
    "    model_id=\"meta.llama2-13b-chat-v1\", \n",
    "    client=bedrock_runtime_client,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "claude3_llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", \n",
    "    client=bedrock_runtime_client,\n",
    "    verbose=True,\n",
    "    model_kwargs={\"temperature\": 0.0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2c54e-946f-484c-9f7a-2a9b72cc072b",
   "metadata": {},
   "source": [
    "## Interfacing with a database via Langchain SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d54f4-bd4f-45d9-a5aa-6c85314ebc78",
   "metadata": {},
   "source": [
    "In this section you will build the foudational element for a tool you will create later, which will take a natural language query and return a response from the in-memory SQLite database.\n",
    "\n",
    "You'll accomplish this by creating a Langchain SQLDatabase chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60f00101-10b9-4f01-aae2-f7d05210552c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain_experimental.sql import SQLDatabaseSequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342fb6e2-5aa8-46bd-af5a-4e45fb50780e",
   "metadata": {},
   "source": [
    "Below you can see an example prompt for your SQLDatabaseChain.\n",
    "\n",
    "It accepts:\n",
    "- `dialect`: parameter for the dialect of the target database\n",
    "- `table_info`: parameter that will autogenerate a sample of the schemas and first few rows of data from the tables\n",
    "- `input`: parameter for the input NLP query to generate SQL from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e86ea3cc-2386-4723-96ca-96a49312162e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"\n",
    "You are a {dialect} expert. Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per {dialect}. You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\\\") to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use date('now') function to get the current date, if the question involves \\\"today\\\".\n",
    "\n",
    "Use the following format:\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use the following tables enclosed in <tables> and </tables> tags:\n",
    "<tables>\n",
    "{table_info}\n",
    "</tables>\n",
    "\n",
    "Once you have a SQLResult, use it to formulate a natural language answer. <example>Answer: The policy id is 12345.</example>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"dialect\", \"top_k\"], template=_DEFAULT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4961fd-6d9d-4d73-a624-e55300b4cc7e",
   "metadata": {},
   "source": [
    "Create a SQLDatabase object from the DB engine created earlier for the SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78da1902-468d-46db-8612-07a1ddf8cfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = SQLDatabase(engine=sqllite_engine, include_tables=[\"policies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201ea63-c676-4627-a556-aae8cee0ec13",
   "metadata": {},
   "source": [
    "You can test your SQLDatabaseChain with a sample NLP query that will be the foundation of a later example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a26699b3-7464-4db8-bc64-343b7914eccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_chain = SQLDatabaseChain.from_llm(llm=llama2_llm, db=db, verbose=True, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b93cf-59f1-4fb0-8606-26d0ce5ebeff",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can test your SQLDatabaseChain with a sample NLP query that will be the foundation of a later example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2bb5911-51be-4e1e-88f2-97faee7df2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain.debug=False\n",
    "sql_chain.invoke(input=\"What is Jewel Ingram's policy id?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe082a5-7f0f-48ee-83fa-b98fcfe7af24",
   "metadata": {},
   "source": [
    "After running the chain, you can see that it returns a policy id for the person in question. Sometimes you may see that the resulting query is not specfic enough (maybe only looking at `first_name` instead of both `first_name` and `last_name`), but this can be addressed by refining the prompt.\n",
    "\n",
    "However, it will suffice for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1998fa-aeee-40f0-b3b5-ba46a21d3ed1",
   "metadata": {},
   "source": [
    "---\n",
    "## Setting up Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec956d-eeaa-4963-8f1e-4dea99eac2cd",
   "metadata": {},
   "source": [
    "With the foundational components now in place, you will begin setting up the concept of tools. Tools are purpose built components to satisfy a particular need. They can look up data, run API commands, or even go perform searches on the internet to satisfy the needs of the reasoning chain of thought.\n",
    "\n",
    "As you proceed you'll see how to built an extensible toolbox that will attempt to automatically select the right tools based on the input query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda280d-008f-445d-a2bb-580701515c3b",
   "metadata": {},
   "source": [
    "### Create API Tool\n",
    "\n",
    "Next, we want our LLM to be able to carry out actions using an API that we will provide.  The API has been setup for you as a mock REST API in Amazon API Gateway. \n",
    "\n",
    "This API has PUT and DELETE methods, to accept requests to modify or cancel an insurance policy. The API takes  an input request and returns a successful response.  Our API isn't backed by any functional logic for simplicity in the workshop environment.  However, a real implementation would be backed by a fully functional API. \n",
    "\n",
    "To include the API in our ReAct workflow, we need to create an API tool.  For this, we are using LangChain's [StructuredTool](https://blog.langchain.dev/structured-tools/) which allows us to represent a function as a tool that an agent can easily interface with to perform actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a325208a-c993-40cb-8c09-90664c246e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_url='https://bkochd081f.execute-api.us-west-2.amazonaws.com/Mock-Test/policy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83f16fa0-1284-4d6c-966a-0d6a6d2c7c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "# A structured tool represents an action an agent can take. It wraps any function you provide to let an agent easily interface with it.\n",
    "    \n",
    "def cancel_policy_request(request: str): \n",
    "    \"\"\"Sends a DELETE request to the policy API with the provided policy id\"\"\"\n",
    "    url = request_url\n",
    "    policy_id = request.rsplit(None, 1)[-1]\n",
    "    result = requests.delete(url+\"?DELETE/\"+policy_id)\n",
    "    return f\"Successfully submitted cancel request for policy: {policy_id}, Status: {result.status_code} - {result.text}\"\n",
    "\n",
    "def update_policy_request(request: str):\n",
    "    \"\"\"Sends a PUT request to the policy API with the provided policy id and data to update\"\"\"\n",
    "    url = request_url\n",
    "    policy_id = request[request.find(\"policy\")+len(\"policy\"):].split()[0]\n",
    "    update_field = request[request.find(\"update the\")+len(\"update the\"):].split()[0]\n",
    "    update_data = request.rsplit(None, 1)[-1]\n",
    "    result = requests.put(url+\"?PUT/\"+policy_id, update_field+\"/\"+update_data)\n",
    "    return f\"Successfully submitted update for: {policy_id}, Update for: {update_field}, {update_data}, Status: {result.status_code} - {result.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1f69aef-4c27-46fd-8029-1d6b804c0eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_policy_request = StructuredTool.from_function(update_policy_request)\n",
    "cancel_policy_request = StructuredTool.from_function(cancel_policy_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d009d62-1f9c-4e4e-ac4a-6014c7271aff",
   "metadata": {},
   "source": [
    "Test the newly created tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "575cf842-d513-4abb-9f7e-d26e31b6fbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cancel_policy_request.run('the policy id is 54621')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef1760c7-0787-4f61-af09-dddbf6da41ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_policy_request.run('Submit a request to update the phone number in policy 54621 to 333-321-5622')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c4fae-853d-4c9b-a9fb-75e5cc69c3d4",
   "metadata": {},
   "source": [
    "Let's now take all of the tools that we've created and add them all to the list of tools that will be made available to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c31fc62d-4ba0-4dc0-9d06-f7b1d236bc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "from langchain.agents import load_tools\n",
    "from langchain.tools import Tool\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(llama2_llm, verbose=True)\n",
    "\n",
    "ALL_TOOLS = [\n",
    "        Tool(\n",
    "            name=\"calculator\",\n",
    "            func=llm_math.run,\n",
    "            description=\"Useful when you need to perform mathematical operations.\"\n",
    "        ),    \n",
    "        Tool(\n",
    "            name=\"insurance_policy_lookup\",\n",
    "            func=sql_chain.run,\n",
    "            description=\"Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id. Example: [What is the policy id for Jane Doe?, What is Cynthia Stone's policy?]\"\n",
    "            #description=\"Useful when you need to look up insurance policy information. This tool takes in a full question about customers and their policies and will return a policy id.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"cancel_policy_request\",\n",
    "            func=cancel_policy_request.run,\n",
    "            description=\"Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned. Example: [Submit a request to cancel policy for policy id of 4321]\"\n",
    "            #description=\"Useful when you need to submit a request to cancel an insurance policy. This tool takes in the customer's policy id to be cancelled.  An API response message will be returned.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"update_policy_request\",\n",
    "            func=update_policy_request.run,\n",
    "            description=\"Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.  Example: [Submit a request to update the phone number in policy 4321 to 455-255-5555]\"\n",
    "            #description=\"Useful when you need to submit a request to update an insurance policy. This tool takes in the customer's policy id to be updated as well as data to be updated. An API response will be returned.\"\n",
    "        ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38bfe5-7313-4a4a-9d6d-104c8bb63284",
   "metadata": {},
   "source": [
    "## Dynamic Tool Selection\n",
    "\n",
    "To create a more dynamic way for the reasoning engine to select the right tool based on the input, we'll setup our dynamic tool selector which will be responsible selecting the right tool based in the input using an embedding model and vector store. \n",
    "\n",
    "![dynamictool](./images/DynamicTools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac8478-3d00-46d2-a377-cf11e7f23e41",
   "metadata": {},
   "source": [
    "An [embedding model](https://huggingface.co/blog/getting-started-with-embeddings) takes a text string and converts it into a numerical vector representation, which will allow you to store it in a vector database and query it based on semantic similarity.\n",
    "\n",
    "Here, you will host a local [MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) embedding model, which will generate a set of embeddings to help semantically search for the right tools in the example.\n",
    "\n",
    "**Note: Ignore any \"Error displaying widget: model not found\" errors you may see here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abde9a56-ac00-4c51-b24d-0a0611ea90d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e314c-e06f-4be2-a5ce-0981825e7e75",
   "metadata": {},
   "source": [
    "The vector store method of tool selection in this example uses [Meta's Facebook AI Similarity Search (FAISS)](https://github.com/facebookresearch/faiss) as a vector database. There are many other options available as well.\n",
    "\n",
    "First you will create a set of documents (one for each tool), that you will vectorize and use to determine best match for tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b97abf3-519e-42d2-98f6-91f9639e0c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=t.description, metadata={\"index\": i})\n",
    "    for i, t in enumerate(ALL_TOOLS)\n",
    "]\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b0676-71a0-4f46-829c-104e629614c9",
   "metadata": {},
   "source": [
    "FAISS provides a `from_documents` method for automatically generating embeddings and storing them in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5a4a233-3daf-4c03-ae6d-65e361b20efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a119c3-ce84-4f85-ae38-484175465b5a",
   "metadata": {},
   "source": [
    "In this section you will define the `get_tools` method, which will embed the input query and find relevant documents that are semantically close. This will allow you to take large lists of tools and scope them down to the most relevant ones.\n",
    "\n",
    "You'll use this method to demonstrate tool scoping below and it will also be used in supplying a list of tools to your agent later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68be0fb1-3815-40d1-90b0-3766c4fa813f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "def get_tools(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\n",
    "\n",
    "def print_tools(tools_arr):\n",
    "    for tool in tools_arr:\n",
    "        print(f'{tool}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0a5ba-1650-4de6-8859-42358ceb16c6",
   "metadata": {},
   "source": [
    "Let's see if we can find the right tool for a given input..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3b3a3-c83b-4293-a3a3-58841dcb04fc",
   "metadata": {},
   "source": [
    "Here you get a list of tools for a mathematical input query. Notice that the `calculator` tool is listed first as its description most closely matches the request. \n",
    "\n",
    "You can also control how many results to return by modifying the `get_relevant_documents` request inside of `get_tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c156111-d6af-462b-b77c-9e705bf0431d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_tools(get_tools(\"What is 281728^2?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183f523-4576-4738-9184-715c07b603ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "Alternatively, the `insurance_policy_lookup` tool is a better match for a policy related inquiry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2750fc1-05f1-443f-8b7c-ca18d52a08b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_tools(get_tools(\"What is the policy id for Bob Jones?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef5887-2059-4c23-912c-7556b9836cfe",
   "metadata": {},
   "source": [
    "While the `cancel_policy_request` tool is a better match for a policy cancellation request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed3b9999-0f8f-4cb3-88d6-f21dca18835c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_tools(get_tools(\"Cancel the policy for policy id 4325\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce42cbc-52e4-40f5-b634-5c1bca0502ac",
   "metadata": {},
   "source": [
    "# Agent Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f992b52-3680-43fd-ae1e-e6e87e51a826",
   "metadata": {},
   "source": [
    "With your models ready to go, and your toolbox configured, you're ready to setup your ReAct agent.\n",
    "\n",
    "The agent will take in a ReAct style prompt along with the list of tools to build out the chain of thought reasoning and resulting actions.\n",
    "\n",
    "The Langchain agent initialization takes 3 prompt keyword arguments to customize the agent's prompt:\n",
    "- `prefix`: the beginning of the prompt\n",
    "- \\<list of tools and their descriptions is automatically injected here\\>\n",
    "- `format_instructions`: how to format the intermediary and final responses, this is important in dictating how the chain of thought is created and processed.\n",
    "- `suffix`: The invocation of the agent. This contains the initial input request as well as the conversational chain to determine what the agent has already done and seen.\n",
    "\n",
    "Some template parameters you see below are:\n",
    "- `{tool_names}`: this is a list of the tools that are available to the agent, in name only. This helps direct the agent into what to supply as actions during reasoning. In this case the agent will do this step for you, but in others you may need to extract the names yourself.\n",
    "- `{agent_scratchpad}`: this is important as it documents the activities and obserations that the agent has had throughout its reasoning. Without this, you will normally see the agent endlessly loop.\n",
    "- `{input}`: The input request from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0507421e-b2e8-4749-b1b3-d46632008338",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_AGENT_PROMPT_TEMPLATE = \"\"\"Human: You are an agent tasked with helping look up and modify insurance claims.\n",
    "\n",
    "Given an input request, take a step-by-step approach to find an insurance policy and modify its status.\n",
    "Only use the tools provided. Do NOT assume any information that hasn't been included in the coversation history.\n",
    "When you have completed the task, end your chain of thought and provide a final response to the user.\n",
    "\n",
    "You have access to the following tools:\n",
    "<tools>\n",
    "{tools}\n",
    "</tools>\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "\n",
    "\n",
    "When you have a response to say to the User, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:{agent_scratchpad}\n",
    "Original input: {input}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "AGENT_PROMPT = PromptTemplate(\n",
    "    input_variables=['tools', 'tool_names', 'agent_scratchpad', 'input'], template=_DEFAULT_AGENT_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5abd42ac-7ab1-4450-8ea7-9cb733aa1c09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_tool_calling_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147d016-ec63-4c82-ab04-4386fcc86b11",
   "metadata": {},
   "source": [
    "This is the user request that the agent will process using the provided tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfb3152d-392d-4c44-9588-1d78c2858b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Update the insurance policy for Charles Wise by updating the phone number to 333-321-5622\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4fd87-f421-46a7-8174-3f6357c3a4c4",
   "metadata": {},
   "source": [
    "Here you will set up your agent.\n",
    "\n",
    "initialize_agent has the following configuration:\n",
    "- `agent`: using the [ZERO_SHOT_REACT agent](https://python.langchain.com/docs/modules/agents/agent_types/react#using-zeroshotreactagent)\n",
    "- `agent_kwargs`: variable contains the object with all of the customized prompt information.\n",
    "- `tools`: list of tools from the vector store, dynamically generated from the input query\n",
    "- `llm`: the agent LLM (in this case Claude V2 from Amazon Bedrock)\n",
    "- `verbose`: show all the details for learning purposes\n",
    "- `return_intermediate_steps`: important for retaining all of the thoughts/actions/observations in the agent_scratchpad\n",
    "- `max_iterations`: VERY IMPORTANT for ensuring that your agent doesn't endlessly loop and run away. Prevents the agent from hammering on the associated LLM.\n",
    "- `handle_parsing_errors`: used for dealing with issues parsing the output of a given step, not necessary but catches edge cases in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "687a3b2b-0920-4f4b-8699-761487b25a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = get_tools(query)\n",
    "\n",
    "agent = create_react_agent(llm=claude3_llm, tools=tools, prompt=AGENT_PROMPT)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294923e8-8957-466c-a436-aa01f1bf6717",
   "metadata": {},
   "source": [
    "Finally you can invoke your agent with the input query!\n",
    "\n",
    "For the first run, we will run with debug = False which will show condensed output from the agent. Following this run, we'll run the same query with debug = True if you want to dive deeper into the details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f2415d1-bbc9-48b7-b3d9-34d7f988cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = False\n",
    "\n",
    "agent_executor.invoke({\"input\":query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aead9f-8954-4da1-b439-3056af299598",
   "metadata": {},
   "source": [
    "Let's try to rerun it with debug set to 'true' where you can dive deep into the details on the Thought > Observation > Action workflow. The output will be verbose, but if you follow through you will see the chain start/end and invoke tools to get the information and take action until it reaches a conclusion or hits the `max_iterations`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ecdb4-a575-42b6-b446-b42e7979a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = True\n",
    "\n",
    "agent_executor.invoke({\"input\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af6965-c609-4117-a506-8f0320063a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
